---
title: "Page Load Analysis"
author: "tdresser@chromium.org, maxlg@chromium.org, dproy@chromium.org"
date: "December 18, 2017"
output: 
  html_document:
    fig_width: 12
    fig_height: 10
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
options(scipen=10000)
knitr::opts_knit$set(verbose=TRUE)
library(plotly)
library(tidyverse)
library(DT)
source('main.R')
load(".RData.main")
```

## Gathering Data
* Load the Alexa top 10k on Nexus 5X's.
* Simulate 3G network.
* Both cold, warm and hot Page loads.
* Enable&disable Subresource Filter
* Reasonably detailed main thread attribution.

These are all measured using live sites. This is required to get ads to behave reasonably, though it introduces some variance between runs.

We've excluded pages which didn't reach first Consistently Interactive across all tests.

## Overall Distribution

Each point represents one run on a specific site. In this aggregate view, it's difficult to see any difference between the subresource filter being on or off. The clean, low bands seen in most graphs appear to be pages which 404 in our page set.

```{r, echo=FALSE, warning=FALSE}

plot_totals_jitter +
  theme(strip.text.x = element_text(size = 7),
        strip.text.y = element_text(size = 7))
```

## Overall Distribution (Sampled)
We've included this view to make it easy to pick out sample sites which fall in various places in these distributions.

```{r, echo=FALSE, warning=FALSE}

ggplotly(plot_totals_jitter_sampled +
  labs(title="Totals per point in time<br>&nbsp;", x="&nbsp;<br>Cache temperature", y="Seconds<br>&nbsp;") +
  theme(
        strip.text.x = element_text(size = 7),
        strip.text.y = element_text(size = 7),
        strip.background = element_blank(),
    ), 
  tooltip="text") %>% layout(margin=list(b=60, l=60, t=60, r=60))
```

## Overall Distribution (Density)

The x-axis here is the number of seconds taken to reach each key point in time. The y-axis is density: how common are values around this one?

Turn lines on and off by clicking on them in the legend. It's interesting that the CPU graphs are generally pretty close to unimodal (apart from a lump at the bottom we believe to be 404 pages), but the wall time graphs are strongly multimodal.

Selecting pairs which differ only between the subresource filter being on and off illustrates the difference, which is largest for Consistently Interactive.

```{r, echo=FALSE}
totals$interaction <- interaction(totals$cache_temperature, totals$is_cpu_time, totals$subresource_filter)
plot_totals_density <- totals %>%
  ggplot(aes(value, group = interaction, color=interaction, text=interaction)) +
  geom_line(stat="density") +
  facet_grid(end ~ .) +
  scale_color_manual(values = colorRampPalette(brewer.pal(name="Set1", n = 8))(14)) +
  scale_x_log10()

ggplotly(plot_totals_density, tooltip="text")
```

Table for First Contentful Paint

```{r, echo=FALSE}
datatable(totals %>% 
             filter(end == "First Contentful Paint") %>% 
             group_by(cache_temperature, is_cpu_time, subresource_filter) %>% 
             summarize(mean = mean(value), 
                       percentile_90 = quantile(value, 0.9),
                       percentile_99 = quantile(value, 0.99)),
          rownames = FALSE,
          options = list(pageLength=50))
```

```{r echo=FALSE}
fcp_plots <- get_endpoint_plots("First Contentful Paint")
```

## First Contentful Paint

In the mean, we're essentially solely blocked on resource requests.

otherNetworkActivities refers to resource requests handled on the browser side. We need to figure out if we can split these out more.

We use a hierarchy to attribute time. Of the following, we attribute time to the first activity present.

* Main thread tasks
* Main thread resource requests
* Browser side resource requests

Our CPU time slightly higher for warm than cold loads in the mean. This is because FCP itself is later, giving more time for CPU work.

From the breakdowns we do have, we can see that time spent fetching script and images dominates.

```{r echo=FALSE, warning=FALSE}
ggplotly(fcp_plots$warm_vs_cold, tooltip="text")
```

Zooming into the CPU work shows that most time is spent executing script, and on "resource_loading". I'm not certain what that work is. TODO

## First Contentful Paint | Quantiles
```{r echo=FALSE, warning=FALSE}
ggplotly(fcp_plots$contributors_by_quantile, tooltip = "text")
```

The normalized data shows that the fraction of time spent per category is surprisingly consistent. Pages which are slower tend to have larger scripts, illustrated by both the time spent fetching and executing scripts increasing in those cases. Image loading is much more of an issue on the first load than subsequent loads.

## First Contentful Paint | Normalized by quantiles
```{r echo=FALSE, warning=FALSE}
ggplotly(fcp_plots$contributors_by_quantile_normalized, tooltip="text")
```

```{r echo=FALSE}
ci_plots <- get_endpoint_plots("Consistently Interactive")
```

## Time To Interactive

```{r echo=FALSE, warning=FALSE}
ggplotly(ci_plots$warm_vs_cold, tooltip="text")
```

## Time To Interactive | Quantiles
```{r echo=FALSE, warning=FALSE}
ggplotly(ci_plots$contributors_by_quantile, tooltip = "text")
```

## Time To Interactive | Normalized by quantiles
```{r echo=FALSE, warning=FALSE}
ggplotly(ci_plots$contributors_by_quantile_normalized, tooltip="text")
```

## Important Timestamp Deltas

This shows the mean contribution per contributor per important timestamp. The first column is time from navigation start to first paint.

```{r echo=FALSE, warning=FALSE}
ggplotly(plot_important_times + 
           theme(strip.text.y = element_text(size = 7),
                 axis.text.x = element_text(size = 6)), tooltip="text") 
```

## Per Second Contributors

This shows the mean contribution per contributor per important second.

```{r echo=FALSE, warning=FALSE}
source('per_second.R')
load(".RData.per_second")
ggplotly(plot_per_second, tooltip="text")
```

Below is the mean contribution per contributor per important second, only for the worst 100 sites (measured by wall clock TTI, without the subresource filter, with a cold cache).

Worst 100 sites.

```{r echo=FALSE, warning=FALSE}
ggplotly(plot_per_second_worst100, tooltip="text")
```

Worst 100 sites.

```{r echo=FALSE, warning=FALSE}
ggplotly(plot_per_second_best100, tooltip="text")
```



